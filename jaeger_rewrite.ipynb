{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite of the code associated with the sine wave generator from \"A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the 'echo state network' approach\" by Herbert Jaeger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries....\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import eigs\n",
    "from matplotlib.animation import FuncAnimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#things to note\n",
    "\n",
    "#still need to check if the W_internal is being generated the same way, are you suppossed to get negative values?\n",
    "#is \"tanh\" a built in matlab function? is this is whats being used as the reservoir activation function? its inside of feval in matlab\n",
    "#what is the deal with rows and collumns in matlap, relative to python? Do i need to care, some arrays are transposed in matlab but i dont think i need to\n",
    "#compare compute_teacher calculation of teachCollectMat, are they equivalent?\n",
    "#confrim train_Woutput aligns with wiener_hopf.m (it definetly does not)\n",
    "#does the wiener method also produce a non-zero weight for input-output unit connections? looks like yes, can we just set this to 0 after training without destroying network\n",
    "\n",
    "#questions for the 26th\n",
    "#how should i figure out if my sin wave has completed a cycle?\n",
    "#maybe the other training method is more robust at handling noise? \n",
    "\n",
    "#due to decay, the actually valuable signal is quite weak by the end of the sine wave, meaning that we need larger weights to extract our wave\n",
    "#these massive weights (sometimes millions) blow up any variance to the point where it destroys the signal\n",
    "#we can counter this by adding more units\n",
    "\n",
    "#is there a way to avoid the wave just missing 0 and thus not triggering the tap feedback\n",
    "\n",
    "#CHECK THE WAY YOU HAVE THE INVERSE FUNCTION ERROR BYPASSED TO MAKE SURE IT WORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just going to have all my input/output pairs up here to keep the rest of the code organized.... \n",
    "\n",
    "#regiment1 = off-set square waves\n",
    "input = np.zeros((150,1))\n",
    "output = np.zeros((150,1))\n",
    "input[20:30] = 1\n",
    "output[30:40] = 0.5\n",
    "regiment1 = [input,output]\n",
    "\n",
    "#regiment2 = sine wave from square wave\n",
    "input = np.zeros((52,1))\n",
    "input[1] = 1\n",
    "\n",
    "output = np.zeros((52,1))\n",
    "wave = np.sin(np.linspace(0,(np.pi * 2),50))\n",
    "\n",
    "for i in range(50):\n",
    "    output[i+1:] = wave[i]\n",
    "\n",
    "output = output * 0.5\n",
    "output[50] = 0.0001\n",
    "\n",
    "regiment2 = [input,output]\n",
    "\n",
    "#regiment 3, two sine waves of different periods from a square wave and a context signal\n",
    "input = np.zeros((200,2))\n",
    "input[5,1] = 1\n",
    "input[100,1] = 2\n",
    "\n",
    "input[5,0] = 1\n",
    "input[100,0] = 1\n",
    "\n",
    "\n",
    "output=np.zeros((200,1))\n",
    "\n",
    "\n",
    "wave = np.sin(np.linspace(0,np.pi*2,50))*0.5\n",
    "wave2 = np.sin(np.linspace(0,np.pi*2,75))*0.5\n",
    "\n",
    "for i in range(50):\n",
    "    output[i+5,0] = wave[i]\n",
    "\n",
    "for i in range(75):\n",
    "    output[i+100,0] = wave2[i]\n",
    "\n",
    "regiment3 = [input,output]\n",
    "\n",
    "#regiment 4, shorter version of regiment 3 to save time\n",
    "input = np.zeros((200,2))\n",
    "input[0,1] = 1\n",
    "input[100,1] = 2\n",
    "\n",
    "input[0,0] = 1\n",
    "input[100,0] = 1\n",
    "\n",
    "\n",
    "output=np.zeros((350,1))\n",
    "\n",
    "\n",
    "wave = np.sin(np.linspace(0,np.pi*2,50))*0.5\n",
    "wave2 = np.sin(np.linspace(0,np.pi*2,100))*0.5\n",
    "\n",
    "for i in range(50):\n",
    "    output[i,0] = wave[i]\n",
    "\n",
    "for i in range(100):\n",
    "    output[i+100,0] = wave2[i]\n",
    "\n",
    "regiment4 = [input,output]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESNN:\n",
    "    def __init__(self, n_input, n_internal, n_output,spectralRadius=1,inputScaling=1,inputShift=0,outputActivationFunction=\"tanh\",noise_level=0, teacherScaling=1, teacherShift=0.0,methodWeightCompute=\"pseudoinverse\", connectivityScale = 1, network_type = \"plain\", atime_constant=1,ltime_constant=1, leakage=0):\n",
    "        import numpy as np\n",
    "\n",
    "        #initialize hyperparameters\n",
    "        self.n_input = n_input\n",
    "        self.n_internal = n_internal\n",
    "        self.n_output = n_output\n",
    "        self.n_total = n_input + n_internal + n_output\n",
    "        #how much are we shifting/scaling the input signal\n",
    "        self.inputScaling = inputScaling\n",
    "        self.inputShift = inputShift\n",
    "        #how much are we shifting/scaling the teacher signal\n",
    "        self.teacherScaling = teacherScaling\n",
    "        self.teacherShift = teacherShift\n",
    "        #spectral radius controls the magnitude of the values in the internal connectivity matrix\n",
    "        self.spectralRadius = spectralRadius\n",
    "        self.noise_level = noise_level\n",
    "        #how are we normalizing outputs to [0,1]\n",
    "        self.outputActivationFunction = outputActivationFunction\n",
    "        self.learningMode = \"offline_singleTimeSeries\"\n",
    "        #what algorithm are we using to compute the output weights\n",
    "        self.methodWeightCompute = methodWeightCompute\n",
    "        self.internalstore = 0\n",
    "        #are the units in the network leaky or not?\n",
    "        self.network_type = network_type\n",
    "\n",
    "        #these are important if leaky\n",
    "        self.atime_constant = atime_constant\n",
    "        self.ltime_constant = ltime_constant\n",
    "        self.leakage = leakage \n",
    "\n",
    "        #intialize hidden weight matrix density\n",
    "    \n",
    "        self.connectivity = min([10/n_internal,1]) * connectivityScale\n",
    "\n",
    "        #generate input weight matrix\n",
    "        self.W_input = 2 * np.random.rand(n_internal, n_input) - 1 \n",
    "        #generate internal weight matrix\n",
    "        self.W_internal = self.gen_W_internal(self.n_internal, self.connectivity)\n",
    "        #generate output weights, original code included input-output connections, lets skip those\n",
    "        self.W_output = 2 * np.random.rand(n_output, n_internal) - 1 \n",
    "        #these are the \"feedback\" weights from output to internal, setting to zero for now, only keeping in code to maintain similar structure as original\n",
    "        self.W_feedback = np.zeros((n_internal, n_output))\n",
    "        #self.W_feedback = 2 * np.random.rand(n_internal, n_output) - 1 \n",
    "\n",
    "        #using this method because I cannot program properly...\n",
    "        self.statemat = []\n",
    "\n",
    "    #the non-linearity that all the unit activity is being fed through\n",
    "    #since we are using the ESNN as a model of the cortex, all activity must be restricted to be above 0, it will just be a hyperbolic tan function shifted upwards\n",
    "    def non_lin(self,activations):\n",
    "\n",
    "        filtered = np.tanh(4*(activations-0.5)) + 1\n",
    "        \n",
    "        ## MAKE SURE THAT THIS FUNCTION IS IN LINE WITH THE INVERSE NON-LIN FUNCTION OR ELSE THIS WHOLE THING BREAKS\n",
    "        return filtered\n",
    "    \n",
    "    #in the learning protocal we need to undo the non-linearity to see what the activation should be, this function performs that\n",
    "    def inverse_non_lin(self,filtered):\n",
    "\n",
    "        unfiltered = 0.25 * np.arctanh(filtered-1)+0.5\n",
    "        return unfiltered \n",
    "\n",
    "\n",
    "    #get this checked next to the matlab code\n",
    "    #this function creates the random internal weight matrix\n",
    "    def gen_W_internal(self, n_internal, connectivity):\n",
    "        #import libraries (get this fixed)\n",
    "        from scipy.sparse.linalg import eigs\n",
    "        import scipy.sparse as sparse\n",
    "        #generate randomly distributed sparese n_internal x n_internal matrix \n",
    "        W_internal = sparse.rand(n_internal, n_internal,density=connectivity,format=\"coo\")\n",
    "        #find the maximum eigenvalue of said matrix\n",
    "        maxval = max(abs(eigs(W_internal, return_eigenvectors=False,k=1,which=\"LM\")))\n",
    "        #transform into an array form (some weird scipy thing i guess)\n",
    "        W_internal = W_internal.toarray()\n",
    "\n",
    "        #center mean of entries at 0, ensure we have negative values to yield echoes\n",
    "        W_internal = np.where(W_internal>0, W_internal - 0.5, W_internal)\n",
    "\n",
    "        #normalize the weight matrix by dividing by the largest eigenvalue, then multiple by the selected spectral radius\n",
    "        return np.array(self.spectralRadius * W_internal / maxval)\n",
    "    \n",
    "    #inputSequence is nTrainingPoints x nInputDim\n",
    "    #outputSequence is nTrainingPoints x nOutputDim\n",
    "    #nForgetPoints is the number of initial data points we wash out to avoid initial transients\n",
    "    def compute_statematrix(self,inputSequence,outputSequence=[],nForgetPoints=0):  \n",
    "        import numpy as np\n",
    "        #if there is no supplied output sequence, the ESNN is assumed to be in testing, as opposed to training\n",
    "        if len(outputSequence) == 0:\n",
    "            teacherForcing = 0\n",
    "            nDataPoints = len(inputSequence[:,0])\n",
    "        else:\n",
    "            teacherForcing = 1\n",
    "            nDataPoints = len(inputSequence[:,0])\n",
    "\n",
    "\n",
    "        #define the \"state collection matrix\", contains all activity of the input as well as internal units\n",
    "        if nForgetPoints > 0:\n",
    "            stateCollectMat = np.zeros((nDataPoints - nForgetPoints, self.n_internal))\n",
    "        else:\n",
    "            stateCollectMat = np.zeros((nDataPoints, self.n_internal))\n",
    "\n",
    "        ### I am ignoring the section of code about giving the network a starting state before input\n",
    "\n",
    "        #matrices holding the activity of all units and internal units?\n",
    "        totalstate = np.zeros(self.n_total) #note the units appear in this matrice in order of [internal, input, output], idk why blame jaeger\n",
    "        internalstate = self.non_lin(np.zeros(self.n_internal))\n",
    "\n",
    "        collectIndex = 0\n",
    "        for i in range(0,nDataPoints):\n",
    "            #shift and scale input activation by predefined values\n",
    "            a_input = inputSequence[i,:] * self.inputScaling + self.inputShift\n",
    "            #write this input signal into the totalstate matrix\n",
    "            totalstate[self.n_internal:self.n_input+self.n_internal] = a_input\n",
    "\n",
    "            #combine all weight matrices into one so matmul works\n",
    "            W_concatenate = np.concatenate((self.W_internal, self.W_input, self.W_feedback),axis=1)\n",
    "            #calculate internal state based on activity of all units, sent through a hyperbolic function to keep below 0\n",
    "            #add uniformly distributed noise\n",
    "            if self.network_type == \"plain\":\n",
    "                internalstate = self.non_lin(np.matmul(W_concatenate,  totalstate)) + np.random.uniform(-1,1,self.n_internal) * self.noise_level\n",
    "            if self.network_type == \"leaky\":\n",
    "                #grab the activity of the internal units last time step\n",
    "                previous_internalstate = totalstate[0:self.n_internal] \n",
    "                #first calculate how much the previous state has decayed, then add the activity resulting from all inputs\n",
    "                internalstate = (1 - self.leakage * self.ltime_constant) * previous_internalstate + self.atime_constant * self.non_lin(np.matmul(W_concatenate,  totalstate))\n",
    "                #add noise \n",
    "                internalstate = internalstate + np.random.uniform(-1,1,self.n_internal) * self.noise_level\n",
    "\n",
    "            #if we are forcing output, the output is set as the predefined signal\n",
    "            #otherwise we calculate it based on internal unit activity\n",
    "            if teacherForcing == 1: \n",
    "                netOut = self.teacherScaling * outputSequence[i,:] + self.teacherShift\n",
    "            else:\n",
    "                netOut = np.tanh(np.matmul(self.W_output, internalstate ))\n",
    "\n",
    "            #put all the unit activities together in a array of form [internal units, input units, output units]\n",
    "            totalstate = np.concatenate((internalstate, a_input, netOut))                \n",
    "\n",
    "            #put all of our input and internal activities into a matrix that contains all their behaviour over time\n",
    "            #we are ignoring the first nForgetPoints entries\n",
    "            if i >= nForgetPoints:\n",
    "                stateCollectMat[collectIndex,:] = internalstate\n",
    "                collectIndex += 1\n",
    "            \n",
    "            if len(self.statemat)==0:\n",
    "                self.statemat = stateCollectMat\n",
    "\n",
    "        return stateCollectMat\n",
    "    \n",
    "    \n",
    "    #this function takes in a input and desired produced output, as well as the number of initial activity points that should be ignored while calculating the statematrix (these may be a problem)\n",
    "    def train(self, trainInput, trainOuput, nForgetPoints):\n",
    "        \n",
    "        #allowing myself to add other training modes later if i want\n",
    "        if self.learningMode == \"offline_singleTimeSeries\":\n",
    "            #computes the activity of the internal units due to input and feedback activity\n",
    "            stateCollection = self.compute_statematrix(inputSequence=trainInput, outputSequence=trainOuput,nForgetPoints=nForgetPoints)\n",
    "            #calculates what the input to the output units should be, \"unsquashes\" them\n",
    "            teacherCollection = self.compute_teacher(outputSequence=trainOuput, nForgetPoints=nForgetPoints)\n",
    "            #actually trains the output weights \n",
    "            self.W_output = self.train_Woutput(stateCollection,teacherCollection)\n",
    "\n",
    "            self.imean_activity = np.mean(self.statemat,axis=1)\n",
    "\n",
    "        return None\n",
    "\n",
    "    #scales, shifts, and applies the inverse output activation function on the expected teacher\n",
    "    def compute_teacher(self,outputSequence,nForgetPoints):\n",
    "        import numpy as np\n",
    "\n",
    "        nOutputPoints = len(outputSequence[:,0])\n",
    "        teachCollectMat = np.zeros((nOutputPoints-max([0,nForgetPoints]),self.n_output))\n",
    "\n",
    "        #remove the first nForgetPoints from the output sequence\n",
    "        if nForgetPoints > 0:\n",
    "            outputSequence = outputSequence[nForgetPoints:,:]\n",
    "\n",
    "        #update the number of point (I'm just doing what Jaeger did dont blame me)\n",
    "        nOutputPoints = len(outputSequence[:,0])\n",
    "\n",
    "        #compare this to matlab code as well, no idea whats happening\n",
    "        #i think this is just shifting/scaling the output, whatever that means\n",
    "        teachCollectMat = self.teacherScaling * outputSequence + self.teacherShift\n",
    "\n",
    "        #calculate what the corresponding input to the output units should be, essentially undo tanh squashing\n",
    "        teachCollectMat=np.arctanh(teachCollectMat)\n",
    "\n",
    "        return teachCollectMat\n",
    "    \n",
    "    #this function actually calculates the new output weights\n",
    "    def train_Woutput(self, stateCollectMat, teacherCollectMat):\n",
    "        import numpy\n",
    "        \n",
    "        #allowing the training method to be specified in case I want to add more later\n",
    "        #i have no clue wtf the wiener-hopf method is, this is my best attempt to replicate the matlab code exactly\n",
    "        if self.methodWeightCompute == \"wiener_hopf\":\n",
    "            runlength = len(stateCollectMat[:,0])\n",
    "            covMat = np.matmul(np.transpose(stateCollectMat),stateCollectMat)/runlength\n",
    "            pVec = np.matmul(np.transpose(stateCollectMat), teacherCollectMat)/runlength\n",
    "            outputWeights = np.matmul(np.linalg.inv(covMat),pVec)\n",
    "\n",
    "        #this is another training method ripped from the matlab code\n",
    "        if self.methodWeightCompute == \"pseudoinverse\":\n",
    "            stateCollectMat = np.linalg.pinv(stateCollectMat)\n",
    "            outputWeights = np.matmul(stateCollectMat,teacherCollectMat)\n",
    "        \n",
    "        else:\n",
    "            print(\"invalid methodWeightCompute value\")\n",
    "\n",
    "\n",
    "        return outputWeights\n",
    "    \n",
    "    #this function will determine how closely the network can replicate the desired output\n",
    "    def evaluate(self, inputSequence, outputSequence,nForgetPoints=0):\n",
    "        import numpy\n",
    "        import matplotlib.pyplot as plt\n",
    "        #generate the expected output of the network\n",
    "        #determine activity of internal and input units\n",
    "        a_internal = self.compute_statematrix(inputSequence=inputSequence, outputSequence=outputSequence, nForgetPoints=nForgetPoints)\n",
    "        #matrix multiply with trained output weights\n",
    "        predictedTrainOutput = np.concatenate(np.tanh(np.matmul(a_internal,self.W_output)))\n",
    "\n",
    "        #remove the shifting and scaling (probably useless)\n",
    "        predictedTrainOutput = (predictedTrainOutput - self.teacherShift)/self.teacherScaling\n",
    "\n",
    "        meanerror, error = self.compute_error(predictedTrainOutput,outputSequence,nForgetPoints)\n",
    "\n",
    "        print(\"mean square error=\", meanerror, \"error=\", error)\n",
    "\n",
    "        #graph our expected and observed signals\n",
    "        y = len(predictedTrainOutput)\n",
    "\n",
    "        mean_activity = numpy.mean(self.statemat,axis=1)\n",
    "\n",
    "        plt.plot(predictedTrainOutput, label=\"Observed Signal\", color=\"red\",alpha = 0.5)\n",
    "        plt.plot(outputSequence,label=\"Training Data\",color=\"blue\", alpha=0.5)\n",
    "        plt.plot(inputSequence,label=\"Input Sequence\", color=\"green\",alpha=0.5)\n",
    "        plt.plot(mean_activity,label=\"Mean Unit Activty\", color=\"darkviolet\",alpha=0.5)\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        print(\"mean activity=\",mean_activity[75:80])\n",
    "\n",
    "        return None \n",
    "\n",
    "    #this function may not work for outputs containing for than one unit (hopefully not a problem)\n",
    "    def compute_error(self, observed, expected, nForgetPoints):\n",
    "\n",
    "        #how long is our real output\n",
    "        nEstimatePoints = len(observed)\n",
    "\n",
    "        nForgetPoints = len(expected[:,0]) - nEstimatePoints\n",
    "\n",
    "        expected = expected[nForgetPoints:]\n",
    "\n",
    "        #three lines above this only relavant if nForgetPoints>0, not sure if this will ever happen\n",
    "\n",
    "        meanerror = np.sum(np.square(expected - observed)) / nEstimatePoints \n",
    "        error = (np.sqrt(meanerror/np.var(expected)))\n",
    "        \n",
    "        return meanerror, error \n",
    "    \n",
    "    def display_units(self):\n",
    "        fig, axs = plt.subplots(4, 5)\n",
    "        storage = []\n",
    "        for i in range(4):\n",
    "            for j in range(5):\n",
    "                passing = False\n",
    "                while passing == False:\n",
    "                    k = np.random.randint(len(self.statemat[0]))\n",
    "                    if storage.count(k)==0:\n",
    "                        storage.append(k)\n",
    "                        axs[i,j].plot(self.statemat[:,k],color = \"darkviolet\",alpha=0.5 )\n",
    "                        passing = True\n",
    "        \n",
    "        for ax in axs.flat:\n",
    "            ax.set(xlabel='time', ylabel='activity')\n",
    "        for ax in axs.flat:\n",
    "            ax.label_outer()\n",
    "\n",
    "        return None\n",
    "\n",
    "    #going off of the pre-exhisting code from compute_statematrix, we will have the ESNN generate its own input based on when its produced sine wave finishes a cycle\n",
    "    #will require specially constructed input sequences\n",
    "    #probably always keep nForgetPoints = 0, i have never tested it so it probably doesn't work, it was in Jaeger's code so I moved it over \n",
    "    def live_run(self, input_type, time_points=0):  \n",
    "        import numpy as np\n",
    "\n",
    "        #does this network take a context signal to produce its sine wave? type 1 means it does not\n",
    "        if input_type==1:\n",
    "            inputSequence  = np.zeros((time_points, 1))\n",
    "            inputSequence[1] = 1\n",
    "            #for i in np.arange(0,time_points,200):\n",
    "                #inputSequence[i] = 1\n",
    "        \n",
    "        if input_type==2:\n",
    "            inputSequence = np.zeros((time_points, 2))\n",
    "            inputSequence[5,0] = 1\n",
    "            inputSequence[5,1] = 1 \n",
    "            \n",
    "        outputSequence = []\n",
    "        nForgetPoints = 0\n",
    "        \n",
    "        #if there is no supplied output sequence, the ESNN is assumed to be in testing, as opposed to training\n",
    "        if len(outputSequence) == 0:\n",
    "            teacherForcing = 0\n",
    "            nDataPoints = len(inputSequence[:,0])\n",
    "        else:\n",
    "            teacherForcing = 1\n",
    "            nDataPoints = len(inputSequence[:,0])\n",
    "\n",
    "\n",
    "        #define the \"state collection matrix\", contains all activity of the input as well as internal units\n",
    "        if nForgetPoints > 0:\n",
    "            stateCollectMat = np.zeros((nDataPoints - nForgetPoints, self.n_internal))\n",
    "        else:\n",
    "            stateCollectMat = np.zeros((nDataPoints, self.n_internal))\n",
    "\n",
    "        outputCollectMat = np.zeros((nDataPoints, self.n_output))\n",
    "\n",
    "        ### I am ignoring the section of code about giving the network a starting state before input\n",
    "\n",
    "        #matrices holding the activity of all units and internal units?\n",
    "        totalstate = np.zeros(self.n_total) #note the units appear in this matrice in order of [internal, input, output], idk why blame jaeger\n",
    "        internalstate = self.non_lin(np.zeros(self.n_internal))\n",
    "\n",
    "        collectIndex = 0\n",
    "        spike_count = 1\n",
    "\n",
    "\n",
    "        #define the refractory period\n",
    "        r_per = 30\n",
    "        r_count = 0\n",
    "        for i in range(0,nDataPoints):\n",
    "            #shift and scale input activation by predefined values\n",
    "            a_input = inputSequence[i,:] * self.inputScaling + self.inputShift\n",
    "            #write this input signal into the totalstate matrix\n",
    "            totalstate[self.n_internal:self.n_input+self.n_internal] = a_input\n",
    "\n",
    "            #combine all weight matrices into one so matmul works\n",
    "            W_concatenate = np.concatenate((self.W_internal, self.W_input, self.W_feedback),axis=1)\n",
    "            #calculate internal state based on activity of all units, sent through a hyperbolic function to keep below 0\n",
    "            #add uniformly distributed noise\n",
    "            if self.network_type == \"plain\":\n",
    "                internalstate = self.non_lin(np.matmul(W_concatenate,  totalstate)) + np.random.uniform(-1,1,self.n_internal) * self.noise_level\n",
    "            if self.network_type == \"leaky\":\n",
    "                #grab the activity of the internal units last time step\n",
    "                previous_internalstate = totalstate[0:self.n_internal] \n",
    "                #first calculate how much the previous state has decayed, then add the activity resulting from all inputs\n",
    "                internalstate = (1 - self.leakage * self.ltime_constant) * previous_internalstate + self.atime_constant * self.non_lin(np.matmul(W_concatenate,  totalstate))\n",
    "                #add noise \n",
    "                internalstate = internalstate + np.random.uniform(-1,1,self.n_internal) * self.noise_level\n",
    "\n",
    "            #if we are forcing output, the output is set as the predefined signal\n",
    "            #otherwise we calculate it based on internal unit activity\n",
    "            if teacherForcing == 1: \n",
    "                netOut = self.teacherScaling * outputSequence[i,:] + self.teacherShift\n",
    "            else:\n",
    "                netOut = np.tanh(np.matmul(np.transpose(self.W_output), internalstate))\n",
    "\n",
    "            #put all the unit activities together in a array of form [internal units, input units, output units]\n",
    "            totalstate = np.concatenate((internalstate, a_input, netOut))                \n",
    "\n",
    "            #put all of our input and internal activities into a matrix that contains all their behaviour over time\n",
    "            #we are ignoring the first nForgetPoints entries\n",
    "            if i >= nForgetPoints:\n",
    "                stateCollectMat[collectIndex,:] = internalstate\n",
    "                outputCollectMat[collectIndex,:] = netOut\n",
    "                collectIndex += 1\n",
    "            \n",
    "            if input_type == 1: \n",
    "                if outputCollectMat[i-1] < 0:\n",
    "                    if outputCollectMat[i] >= 0:\n",
    "                        if r_count > 0:\n",
    "                            pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                inputSequence[i+1] = 1\n",
    "                                r_count = r_per\n",
    "                                totalstate[0:self.n_internal] =self.non_lin(np.zeros(self.n_internal))\n",
    "                            except:\n",
    "                                pass\n",
    "            \n",
    "            if input_type == 2:\n",
    "                if outputCollectMat[i-1] < 0:\n",
    "                    if outputCollectMat[i] >= 0:\n",
    "                        if r_per > 0:\n",
    "                            pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                inputSequence[i+1,:] = 1\n",
    "                                r_count = r_per\n",
    "                                spike_count+=1\n",
    "                                totalstate[0:self.n_internal] =self.non_lin(np.zeros(self.n_internal))\n",
    "                            except:\n",
    "                                pass\n",
    "                            if spike_count > 3:\n",
    "                                try:\n",
    "                                    inputSequence[i+1,1] = 2\n",
    "                                    r_count = r_per\n",
    "                                    totalstate[0:self.n_internal] =self.non_lin(np.zeros(self.n_internal))\n",
    "                                except:\n",
    "                                    pass\n",
    "            r_count = r_count - 1 \n",
    "            print(r_count)\n",
    "            self.statemat = stateCollectMat\n",
    "        return stateCollectMat, outputCollectMat, inputSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn = ESNN(n_input=1,n_internal=2000,n_output=1,spectralRadius=4,inputScaling=1,noise_level=0.000000001, methodWeightCompute=\"pseudoinverse\", connectivityScale = 1,teacherShift=0)\n",
    "\n",
    "esn.train(regiment1[0],regiment1[1],0)\n",
    "esn.evaluate(regiment1[0],regiment1[1])\n",
    "esn.display_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn2 = ESNN(n_input=1,n_internal=2000,n_output=1,spectralRadius=2.2,inputScaling=1,noise_level=0.0, methodWeightCompute=\"pseudoinverse\", connectivityScale=1, network_type =\"leaky\",leakage=0.5,atime_constant=1,ltime_constant=1)\n",
    "\n",
    "\n",
    "esn2.train(regiment2[0],regiment2[1],0)\n",
    "esn2.evaluate(regiment2[0],regiment2[1])\n",
    "esn2.display_units()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn3 = ESNN(n_input=2,n_internal=18000,n_output=1,spectralRadius=5.22,inputScaling=[1, 0.01],noise_level=0.0, methodWeightCompute=\"pseudoinverse\", connectivityScale=1)\n",
    "esn3.train(regiment3[0],regiment3[1],0)\n",
    "esn3.evaluate(regiment3[0],regiment3[1])\n",
    "esn3.display_units()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal, output,input = esn2.live_run(time_points = 200,input_type=1)\n",
    "plt.plot(output, color=\"red\", alpha = 0.5, label = \"output\")\n",
    "plt.plot(input, color=\"blue\", alpha = 0.5, label = \"input\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal,output,input = esn3.live_run(time_points=300,input_type=2)\n",
    "plt.plot(output, color=\"red\", alpha = 0.5, label = \"output\")\n",
    "plt.plot(input, color=\"blue\", alpha = 0.5, label = \"input\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn3.display_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esn4 = ESNN(n_input=2,n_internal=2000,n_output=1,spectralRadius=3.05,inputScaling=[1,0.001],noise_level=0.0, methodWeightCompute=\"pseudoinverse\", connectivityScale=1, network_type =\"leaky\",leakage=1,atime_constant=1,ltime_constant=0.75)\n",
    "esn4.train(regiment3[0],regiment3[1],0)\n",
    "\n",
    "esn4.evaluate(regiment3[0],regiment3[1])\n",
    "esn4.display_units()\n",
    "print(np.mean(esn4.W_output))\n",
    "print(np.mean(abs(esn4.statemat[100,:])))\n",
    "#52000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal,output,input = esn4.live_run(time_points=300,input_type=2)\n",
    "plt.plot(output, color=\"red\", alpha = 0.5, label = \"output\")\n",
    "plt.plot(input, color=\"blue\", alpha = 0.5, label = \"input\")\n",
    "\n",
    "esn4.display_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
